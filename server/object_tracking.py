import base64
import json
import sys
import tempfile
import os
from typing import Any, List, Dict
from pathlib import Path

import cv2
import numpy as np

try:
    from ultralytics import YOLO
except ImportError as exc:
    raise SystemExit(
        json.dumps({
            "success": False,
            "error": "Required packages not installed",
            "details": str(exc),
            "install_command": "pip install ultralytics opencv-python numpy"
        })
    )

# Note: YOLO (ultralytics) has built-in tracking using ByteTrack algorithm
# No need to install byte-track separately - it's integrated in ultralytics
BYTETRACKER_AVAILABLE = False  # We use YOLO's built-in tracking

# Try to import SAM3, but make it optional
# SAM3 is only needed if user wants segmentation (useSAM3=true)
try:
    from sam3 import SAM3
    SAM3_AVAILABLE = True
except ImportError:
    SAM3_AVAILABLE = False
    # Don't print warning - SAM3 is optional and not needed for basic tracking


def to_serializable(value: Any):
    """Convert numpy types and other non-serializable objects to JSON-compatible types."""
    if isinstance(value, dict):
        # Convert keys to strings if they are numpy types
        result = {}
        for k, v in value.items():
            # Convert numpy key to Python native type
            if isinstance(k, (np.integer, np.int64, np.int32)):
                k = int(k)
            elif isinstance(k, (np.floating, np.float64, np.float32)):
                k = float(k)
            elif not isinstance(k, (str, int, float, bool)) and k is not None:
                k = str(k)
            result[k] = to_serializable(v)
        return result
    if isinstance(value, (list, tuple)):
        return [to_serializable(v) for v in value]
    if isinstance(value, np.ndarray):
        return value.tolist()
    if isinstance(value, (np.generic, np.integer, np.int64, np.int32, np.floating, np.float64, np.float32)):
        return value.item()
    if isinstance(value, (int, float, str, bool)) or value is None:
        return value
    return str(value)


def process_video_frame_by_frame(
    video_path: str,
    yolo_model_path: str = None,
    frame_skip: int = 1,
    conf_threshold: float = 0.25,
    iou_threshold: float = 0.45,
    use_sam3: bool = False,
    force_mp4v: bool = False,
) -> Dict[str, Any]:
    """
    Process video with object tracking using YOLO + ByteTrack.
    
    Args:
        video_path: Path to input video file
        yolo_model_path: Path to YOLO model file (None = auto-detect custom model)
        frame_skip: Process every Nth frame (1 = all frames)
        conf_threshold: Detection confidence threshold
        iou_threshold: IOU threshold for NMS
        use_sam3: Whether to use SAM3 for segmentation (if available)
    
    Returns:
        Dictionary with tracking results and annotated video
    """
    # Auto-detect model: prefer custom model if exists, otherwise use default
    if yolo_model_path is None:
        script_dir = Path(__file__).parent
        custom_model_path = script_dir / "yolov8s_car_custom.pt"
        default_model_path = script_dir / "yolov8n.pt"
        
        if custom_model_path.exists():
            yolo_model_path = str(custom_model_path)
            print(f"‚úÖ Using custom trained model: {yolo_model_path}", file=sys.stderr)
        elif default_model_path.exists():
            yolo_model_path = str(default_model_path)
            print(f"‚ÑπÔ∏è  Using default model: {yolo_model_path}", file=sys.stderr)
        else:
            yolo_model_path = "yolov8n.pt"  # Will auto-download
            print(f"‚ÑπÔ∏è  Using default model (will download): {yolo_model_path}", file=sys.stderr)
    
    # Load YOLO model
    try:
        model = YOLO(yolo_model_path)
    except Exception as e:
        raise SystemExit(json.dumps({
            "success": False,
            "error": f"Failed to load YOLO model: {str(e)}"
        }))
    
    # Detect model type and adjust classes filter
    # Custom model (trained for car detection) typically has 1 class (class 0)
    # COCO models have 80 classes (car=2, motorcycle=3, bus=5, truck=7)
    num_classes = len(model.names)
    is_custom_model = num_classes == 1 or "custom" in yolo_model_path.lower()
    
    if is_custom_model:
        # Custom model: detect all classes (usually just class 0 = car)
        classes_filter = None  # None means detect all classes
        print(f"‚ÑπÔ∏è  Custom model detected ({num_classes} class): detecting all classes", file=sys.stderr)
    else:
        # COCO model: filter vehicle classes
        classes_filter = [2, 3, 5, 7]  # car, motorcycle, bus, truck
        print(f"‚ÑπÔ∏è  COCO model detected ({num_classes} classes): filtering vehicle classes {classes_filter}", file=sys.stderr)
    
    # YOLO (ultralytics) has built-in ByteTrack algorithm
    # No need to initialize separate tracker - YOLO's track() method handles it
    print("‚ÑπÔ∏è  Using YOLO built-in ByteTrack tracking", file=sys.stderr)
    
    # Initialize SAM3 if requested and available
    sam3_model = None
    if use_sam3 and SAM3_AVAILABLE:
        try:
            sam3_model = SAM3()
            print("‚úÖ SAM3 initialized", file=sys.stderr)
        except Exception as e:
            print(f"‚ö†Ô∏è  Failed to initialize SAM3: {e}", file=sys.stderr)
            use_sam3 = False
    
    # Open video
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        raise SystemExit(json.dumps({
            "success": False,
            "error": f"Failed to open video: {video_path}"
        }))
    
    fps = int(cap.get(cv2.CAP_PROP_FPS)) or 30
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    
    # Create output video writer
    # Default: try H.264 then fallback mp4v; if force_mp4v=True ‚Üí d√πng mp4v lu√¥n
    output_path = os.path.join(tempfile.gettempdir(), f"tracked_{os.path.basename(video_path)}")
    if force_mp4v:
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
    else:
        # Try H.264 first, fallback to mp4v
        fourcc = cv2.VideoWriter_fourcc(*'avc1')  # H.264 codec
        try:
            out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
            if not out.isOpened():
                fourcc = cv2.VideoWriter_fourcc(*'mp4v')
                out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
        except Exception:
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
    
    frame_count = 0
    processed_frames = 0          # t·ªïng frame ƒë√£ ghi ra
    detected_frames = 0           # s·ªë frame ch·∫°y YOLO
    all_tracks: List[Dict[str, Any]] = []
    track_history: Dict[int, List[Dict[str, Any]]] = {}  # track_id -> list of positions
    
    print(f"üìπ Processing video: {width}x{height} @ {fps}fps, {total_frames} frames", file=sys.stderr)
    
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        
        do_detect = (frame_count % frame_skip == 0)
        detections = []

        if do_detect:
            # Run YOLO detection with tracking
            track_kwargs = {
                "persist": True,  # ByteTrack inside
                "conf": conf_threshold,
                "iou": iou_threshold,
                "verbose": False
            }
            if classes_filter is not None:
                track_kwargs["classes"] = classes_filter

            results = model.track(frame, **track_kwargs)

            if results[0].boxes is not None:
                boxes = results[0].boxes.xyxy.cpu().numpy()
                scores = results[0].boxes.conf.cpu().numpy()
                classes = results[0].boxes.cls.cpu().numpy().astype(int)
                track_ids = results[0].boxes.id
                if track_ids is not None:
                    track_ids = track_ids.cpu().numpy().astype(int)
                else:
                    track_ids = np.array([-1] * len(boxes))
                
                for i, (box, score, cls, tid) in enumerate(zip(boxes, scores, classes, track_ids)):
                    x1, y1, x2, y2 = map(int, box)
                    track_id = None
                    if tid is not None and tid >= 0:
                        track_id = int(tid)
                    
                    detections.append({
                        'bbox': [int(x1), int(y1), int(x2 - x1), int(y2 - y1)],
                        'confidence': float(score),
                        'class': int(cls),
                        'class_name': str(model.names[int(cls)]),
                        'track_id': track_id
                    })
                    
                    if tid >= 0:
                        track_id_key = int(tid)
                        if track_id_key not in track_history:
                            track_history[track_id_key] = []
                        center_x = (x1 + x2) // 2
                        center_y = (y1 + y2) // 2
                        track_history[track_id_key].append({
                            'frame': int(frame_count),
                            'center': [int(center_x), int(center_y)],
                            'bbox': [int(x1), int(y1), int(x2), int(y2)]
                        })

            detected_frames += 1

        # Draw annotations (only detections available on detect frames)
        annotated_frame = frame.copy()
        for det in detections:
            x, y, w, h = det['bbox']
            x1, y1, x2, y2 = x, y, x + w, y + h
            track_id = det['track_id']
            class_name = det['class_name']
            confidence = det['confidence']
            
            color = (
                int((track_id * 50) % 255),
                int((track_id * 100) % 255),
                int((track_id * 150) % 255)
            ) if track_id is not None else (0, 255, 0)
            
            cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), color, 2)
            
            label = f"ID:{track_id} {class_name}" if track_id is not None else f"{class_name}"
            label += f" {confidence:.2f}"
            (tw, th), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)
            cv2.rectangle(annotated_frame, (x1, y1 - th - 8), (x1 + tw + 6, y1), color, -1)
            cv2.putText(
                annotated_frame,
                label,
                (x1 + 3, y1 - 5),
                cv2.FONT_HERSHEY_SIMPLEX,
                0.6,
                (255, 255, 255),
                2
            )
            
            if track_id is not None and track_id in track_history:
                trail = track_history[track_id][-10:]
                if len(trail) > 1:
                    points = [t['center'] for t in trail]
                    for i in range(1, len(points)):
                        cv2.line(
                            annotated_frame,
                            tuple(points[i-1]),
                            tuple(points[i]),
                            color,
                            2
                        )

        out.write(annotated_frame)

        # L∆∞u k·∫øt qu·∫£ ch·ªâ cho frame c√≥ detect (ƒë·ªÉ tr√°nh ph√¨nh d·ªØ li·ªáu)
        if do_detect:
            all_tracks.append({
                'frame': int(frame_count),
                'timestamp': float(frame_count / fps),
                'detections': detections
            })
        
        processed_frames += 1
        frame_count += 1
        
        # Progress update
        if processed_frames % 10 == 0:
            progress = (frame_count / total_frames) * 100
            print(f"‚è≥ Progress: {progress:.1f}% ({processed_frames} frames processed)", file=sys.stderr)
    
    cap.release()
    out.release()
    
    # Read output video as base64
    with open(output_path, 'rb') as f:
        video_bytes = f.read()
    video_b64 = base64.b64encode(video_bytes).decode('utf-8')
    
    # Use webm or mp4 MIME type based on codec
    # For better browser compatibility, use mp4
    video_mime = 'video/mp4'
    
    # Clean up temp file
    try:
        os.remove(output_path)
    except:
        pass
    
    # Aggregate statistics
    unique_tracks = set()
    for frame_data in all_tracks:
        for det in frame_data['detections']:
            if det['track_id'] is not None:
                # Ensure track_id is Python int, not numpy.int64
                tid = det['track_id']
                if isinstance(tid, (np.integer, np.int64, np.int32)):
                    tid = int(tid)
                unique_tracks.add(tid)
    
    # Ensure all return values are Python native types
    return {
        'success': True,
        'total_frames': int(total_frames),
        'processed_frames': int(processed_frames),
        'unique_tracks': int(len(unique_tracks)),
        'video_width': int(width),
        'video_height': int(height),
        'fps': int(fps),
        'annotatedVideo': f"data:{video_mime};base64,{video_b64}",
        'tracks': to_serializable(all_tracks),
        'track_history': to_serializable(track_history),
        'summary': {
            'total_objects_detected': int(len(unique_tracks)),
            'total_detections': int(sum(len(f['detections']) for f in all_tracks)),
            'avg_detections_per_frame': float(sum(len(f['detections']) for f in all_tracks) / max(processed_frames, 1))
        }
    }


def main():
    """Main entry point for video object tracking."""
    try:
        payload = json.loads(sys.stdin.read() or "{}")
        
        # Get video data (can be base64 or file path)
        video_data = payload.get("videoData")
        video_path = payload.get("videoPath")
        
        if not video_data and not video_path:
            raise SystemExit(json.dumps({
                "success": False,
                "error": "Missing videoData or videoPath"
            }))
        
        # If video_data is provided (base64), save to temp file
        if video_data:
            if "," in video_data:
                video_data = video_data.split(",", 1)[1]
            
            try:
                video_bytes = base64.b64decode(video_data)
                with tempfile.NamedTemporaryFile(delete=False, suffix='.mp4') as tmp:
                    tmp.write(video_bytes)
                    video_path = tmp.name
            except Exception as e:
                raise SystemExit(json.dumps({
                    "success": False,
                    "error": f"Failed to decode video data: {str(e)}"
                }))
        
        # Get optional parameters
        frame_skip = payload.get("frameSkip", 1)
        conf_threshold = payload.get("confThreshold", 0.25)
        iou_threshold = payload.get("iouThreshold", 0.45)
        use_sam3 = payload.get("useSAM3", False) and SAM3_AVAILABLE
        yolo_model_path = payload.get("yoloModelPath", None)  # None = auto-detect
        
        # Process video
        result = process_video_frame_by_frame(
            video_path=video_path,
            yolo_model_path=yolo_model_path,
            frame_skip=frame_skip,
            conf_threshold=conf_threshold,
            iou_threshold=iou_threshold,
            use_sam3=use_sam3
        )
        
        # Clean up temp file if created from base64
        if video_data and os.path.exists(video_path):
            try:
                os.remove(video_path)
            except:
                pass
        
        print(json.dumps(result))
        
    except SystemExit:
        raise
    except Exception as e:
        raise SystemExit(json.dumps({
            "success": False,
            "error": f"Processing error: {str(e)}"
        }))


if __name__ == "__main__":
    main()

