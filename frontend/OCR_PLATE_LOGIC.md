# üöó Smart Parking - OCR Plate Detection Logic

## üìã T·ªïng quan

H·ªá th·ªëng nh·∫≠n d·∫°ng bi·ªÉn s·ªë xe (License Plate Recognition - LPR) s·ª≠ d·ª•ng **fast-alpr** library v·ªõi 2 models:
- **Detector**: `yolo-v9-t-384-license-plate-end2end` - Ph√°t hi·ªán v·ªã tr√≠ bi·ªÉn s·ªë
- **OCR**: `global-plates-mobile-vit-v2-model` - Nh·∫≠n d·∫°ng k√Ω t·ª± tr√™n bi·ªÉn s·ªë

---

## üîÑ **Workflow t·ªïng th·ªÉ**

```
User Capture Frame
    ‚Üì
Frontend: Convert to base64
    ‚Üì
POST /api/plate-detect
    ‚Üì
Backend: Decode base64 ‚Üí OpenCV image
    ‚Üì
ALPR Model: Detect + OCR
    ‚Üì
Backend: Annotate image (draw boxes + text)
    ‚Üì
Response: { plates: [...], annotatedImage: "data:image/..." }
    ‚Üì
Frontend: Save to Firestore (plateDetections collection)
    ‚Üì
Display in UI
```

---

## üñ•Ô∏è **Backend Implementation**

### **1. Standalone Script: `server/plate_detect.py`**

Script ƒë·ªôc l·∫≠p c√≥ th·ªÉ ch·∫°y t·ª´ command line ho·∫∑c ƒë∆∞·ª£c g·ªçi t·ª´ FastAPI.

#### **Input:**
```json
{
  "imageData": "data:image/jpeg;base64,/9j/4AAQSkZJRg..."
}
```

#### **Processing:**

```71:94:server/plate_detect.py
def main():
    payload = json.loads(sys.stdin.read() or "{}")
    image_data = payload.get("imageData")
    if not image_data:
        raise SystemExit(json.dumps({"success": False, "error": "Missing imageData"}))

    if "," in image_data:
        image_data = image_data.split(",", 1)[1]

    try:
        image_bytes = base64.b64decode(image_data)
    except Exception as exc:  # noqa: BLE001
        raise SystemExit(json.dumps({"success": False, "error": "Invalid base64 image", "details": str(exc)}))

    np_array = np.frombuffer(image_bytes, dtype=np.uint8)
    frame = cv2.imdecode(np_array, cv2.IMREAD_COLOR)
    if frame is None:
        raise SystemExit(json.dumps({"success": False, "error": "Unable to decode image"}))

    alpr = ALPR(
        detector_model="yolo-v9-t-384-license-plate-end2end",
        ocr_model="global-plates-mobile-vit-v2-model",
    )
    results = alpr.predict(frame)
```

#### **ALPR Model:**
- **Library**: `fast-alpr` (Python package)
- **Detector**: YOLO v9 tiny 384 - Ph√°t hi·ªán bi·ªÉn s·ªë
- **OCR**: Mobile ViT v2 - Nh·∫≠n d·∫°ng k√Ω t·ª±

#### **Extract Results:**

```103:155:server/plate_detect.py
    for result in results:
        # Check for object attributes directly
        plate_text = getattr(result, "plate", "") or ""
        confidence = getattr(result, "confidence", 0.0)
        detection = getattr(result, "detection", None)

        if not plate_text and hasattr(result, "ocr"):
            # Maybe nested?
            ocr_obj = getattr(result, "ocr", None)
            if ocr_obj:
                plate_text = getattr(ocr_obj, "text", "") or ""
                confidence = getattr(ocr_obj, "confidence", 0.0)

        plate_text = plate_text.upper().strip()

        # Ch·ªâ th√™m plate n·∫øu c√≥ text (kh√¥ng filter theo ƒë·ªô d√†i ƒë·ªÉ kh√¥ng b·ªè s√≥t)
        # B·ªè qua c√°c detection kh√¥ng c√≥ text
        if not plate_text:
            continue  # Skip plates without text

        bbox = [0, 0, 0, 0]
        if detection and hasattr(detection, "box"):
            # [x1, y1, x2, y2]
            box = detection.box
            if len(box) == 4:
                x1, y1, x2, y2 = map(int, box)
                bbox = [x1, y1, x2 - x1, y2 - y1] # Convert to [x, y, w, h] for frontend
                
                # Draw green box
                cv2.rectangle(annotated, (x1, y1), (x2, y2), (64, 255, 120), 3)

                # Draw text background and text inside box bottom
                label = f"{plate_text} ({confidence * 100:.1f}%)"
                (w, h), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.75, 2)
                text_y = max(y2 - 8, y1 + h + 8)
                cv2.rectangle(annotated, (x1, text_y - h - 8), (x1 + w + 12, text_y + 6), (64, 255, 120), -1)
                cv2.putText(
                    annotated,
                    label,
                    (x1 + 6, text_y),
                    cv2.FONT_HERSHEY_SIMPLEX,
                    0.75,
                    (0, 40, 20),
                    2,
                )

        plates.append(
            {
                "text": plate_text,
                "confidence": float(confidence),
                "bbox": bbox,
            }
        )
```

#### **Output:**
```json
{
  "plates": [
    {
      "text": "30A-12345",
      "confidence": 0.95,
      "bbox": [100, 200, 150, 50]
    }
  ],
  "annotatedImage": "data:image/png;base64,..."
}
```

---

### **2. AI Service: `server/services/ai_service.py`**

Class-based service t√≠ch h·ª£p tr·ª±c ti·∫øp v√†o FastAPI (kh√¥ng spawn subprocess).

#### **Initialization:**

```23:35:server/services/ai_service.py
class AIService:
    """AI Service qu·∫£n l√Ω YOLO v√† ALPR models"""
    
    def __init__(self):
        self.yolo_model = None
        self.alpr_model = None
        self.models_loaded = False
        
        # Paths
        self.script_dir = Path(__file__).parent.parent
        self.custom_model_path = self.script_dir / "yolov8s_car_custom.pt"
        self.default_model_path = self.script_dir / "yolov8n.pt"
```

#### **Load Models:**

```36:74:server/services/ai_service.py
    async def load_models(self):
        """Load YOLO v√† ALPR models 1 l·∫ßn duy nh·∫•t"""
        if self.models_loaded:
            return
        
        # Load YOLO model
        try:
            # ∆Øu ti√™n custom model
            if self.custom_model_path.exists():
                model_path = str(self.custom_model_path)
                print(f"‚úÖ Loading custom YOLO model: {model_path}")
            elif self.default_model_path.exists():
                model_path = str(self.default_model_path)
                print(f"‚ÑπÔ∏è  Loading default YOLO model: {model_path}")
            else:
                model_path = "yolov8n.pt"
                print(f"‚ÑπÔ∏è  Downloading YOLO model: {model_path}")
            
            self.yolo_model = YOLO(model_path)
            print(f"‚úÖ YOLO model loaded successfully")
            
        except Exception as e:
            print(f"‚ùå Failed to load YOLO model: {e}")
            raise
        
        # Load ALPR model
        try:
            self.alpr_model = ALPR(
                detector_model="yolo-v9-t-384-license-plate-end2end",
                ocr_model="global-plates-mobile-vit-v2-model",
            )
            print(f"‚úÖ ALPR model loaded successfully")
            
        except Exception as e:
            print(f"‚ùå Failed to load ALPR model: {e}")
            raise
        
        self.models_loaded = True
        print(f"üéâ All AI models loaded and ready!")
```

#### **Detect Plate Method:**

```76:183:server/services/ai_service.py
    async def detect_plate(self, image_data: str) -> Dict[str, Any]:
        """
        Detect license plates trong image
        
        Args:
            image_data: Base64 encoded image string
        
        Returns:
            Dict v·ªõi plates v√† annotated image
        """
        if not self.models_loaded:
            await self.load_models()
        
        # Decode base64 image
        if "," in image_data:
            image_data = image_data.split(",", 1)[1]
        
        try:
            image_bytes = base64.b64decode(image_data)
        except Exception as e:
            raise ValueError(f"Invalid base64 image: {e}")
        
        # Convert to OpenCV format
        np_array = np.frombuffer(image_bytes, dtype=np.uint8)
        frame = cv2.imdecode(np_array, cv2.IMREAD_COLOR)
        
        if frame is None:
            raise ValueError("Unable to decode image")
        
        # Run ALPR prediction
        results = self.alpr_model.predict(frame)
        
        # Annotate image
        annotated = frame.copy()
        plates = []
        
        for result in results:
            # Extract plate info
            plate_text = getattr(result, "plate", "") or ""
            confidence = getattr(result, "confidence", 0.0)
            detection = getattr(result, "detection", None)
            
            plate_text = plate_text.upper().strip()
            
            # Skip empty plates
            if not plate_text:
                continue
            
            # Extract bbox
            bbox = [0, 0, 0, 0]
            if detection and hasattr(detection, "box"):
                box = detection.box
                if len(box) == 4:
                    x1, y1, x2, y2 = map(int, box)
                    bbox = [x1, y1, x2 - x1, y2 - y1]  # [x, y, w, h]
                    
                    # Draw green box
                    cv2.rectangle(annotated, (x1, y1), (x2, y2), (64, 255, 120), 3)
                    
                    # Draw label
                    label = f"{plate_text} ({confidence * 100:.1f}%)"
                    (w, h), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.75, 2)
                    text_y = max(y2 - 8, y1 + h + 8)
                    cv2.rectangle(annotated, (x1, text_y - h - 8), (x1 + w + 12, text_y + 6), (64, 255, 120), -1)
                    cv2.putText(
                        annotated,
                        label,
                        (x1 + 6, text_y),
                        cv2.FONT_HERSHEY_SIMPLEX,
                        0.75,
                        (0, 40, 20),
                        2,
                    )
            
            plates.append({
                "text": plate_text,
                "confidence": float(confidence),
                "bbox": bbox,
            })
        
        # Add banner if plates detected
        if plates:
            banner = f"[{plates[0]['text']}]"
            (tw, th), _ = cv2.getTextSize(banner, cv2.FONT_HERSHEY_SIMPLEX, 1.1, 3)
            bx = max(20, (annotated.shape[1] - tw) // 2 - 20)
            by = annotated.shape[0] - 25
            cv2.rectangle(annotated, (bx - 10, by - th - 15), (bx + tw + 10, by + 15), (255, 255, 255), -1)
            cv2.putText(
                annotated,
                banner,
                (bx, by),
                cv2.FONT_HERSHEY_SIMPLEX,
                1.1,
                (0, 0, 0),
                3,
            )
        
        # Encode annotated image
        ok, buffer = cv2.imencode(".png", annotated)
        if not ok:
            raise RuntimeError("Failed to encode annotated image")
        
        annotated_b64 = base64.b64encode(buffer.tobytes()).decode("utf-8")
        
        return {
            "plates": plates,
            "annotatedImage": f"data:image/png;base64,{annotated_b64}",
        }
```

---

### **3. FastAPI Endpoint: `server/main_fastapi.py`**

```283:307:server/main_fastapi.py
async def detect_license_plate(request: dict):
    """
    Detect license plate t·ª´ image
    Input: { "imageData": "data:image/jpeg;base64,..." }
    """
    try:
        image_data = request.get("imageData")
        if not image_data:
            raise HTTPException(status_code=400, detail="imageData is required")
        
        print(f"üì• Received plate detection request")
        
        # G·ªçi AI service tr·ª±c ti·∫øp (KH√îNG spawn subprocess)
        result = await ai_service.detect_plate(image_data)
        
        # L∆∞u v√†o Firebase
        if result.get("plates"):
            await firebase_service.save_plate_detection(result)
        
        print(f"‚úÖ Detected {len(result.get('plates', []))} plates")
        return {"success": True, **result}
        
    except Exception as e:
        print(f"‚ùå Plate detection error: {e}")
        raise HTTPException(status_code=500, detail=str(e))
```

**Endpoint:** `POST /api/plate-detect`

**Request:**
```json
{
  "imageData": "data:image/jpeg;base64,/9j/4AAQSkZJRg..."
}
```

**Response:**
```json
{
  "success": true,
  "plates": [
    {
      "text": "30A-12345",
      "confidence": 0.95,
      "bbox": [100, 200, 150, 50]
    }
  ],
  "annotatedImage": "data:image/png;base64,..."
}
```

---

## üíª **Frontend Implementation**

### **1. Plate Detection Service: `frontend/src/services/plateDetectionService.ts`**

#### **Data Structure:**

```17:28:frontend/src/services/plateDetectionService.ts
export interface PlateDetectionRecord {
  id: string;
  ownerId: string;
  parkingId: string;
  cameraId: string;
  plateText: string;
  confidence: number;
  inputImageUrl: string;
  annotatedImageUrl?: string;
  rawResponse?: unknown;
  createdAt: Date;
}
```

#### **Save to Firestore:**

```41:68:frontend/src/services/plateDetectionService.ts
export async function savePlateDetection(payload: SavePlateDetectionPayload) {
  try {
    // Filter out undefined values - Firebase doesn't accept undefined
    const firestoreData: Record<string, unknown> = {
      ownerId: payload.ownerId,
      parkingId: payload.parkingId,
      cameraId: payload.cameraId,
      plateText: payload.plateText,
      confidence: payload.confidence,
      inputImageUrl: payload.inputImageUrl,
      createdAt: serverTimestamp(),
    };
    
    // Only add optional fields if they are defined
    if (payload.annotatedImageUrl !== undefined) {
      firestoreData.annotatedImageUrl = payload.annotatedImageUrl;
    }
    if (payload.rawResponse !== undefined) {
      firestoreData.rawResponse = payload.rawResponse;
    }
    
    const docRef = await addDoc(collection(db, COLLECTION), firestoreData);
    return { success: true, id: docRef.id };
  } catch (error) {
    console.error('Failed to save plate detection', error);
    return { success: false, error: error instanceof Error ? error.message : String(error) };
  }
}
```

**Collection:** `plateDetections`

---

### **2. Stream Host Page: `frontend/src/pages/StreamHostPage.tsx`**

#### **Capture & Detect Flow:**

```696:850:frontend/src/pages/StreamHostPage.tsx
  // Handle capture and detect plate number
  const handleCaptureAndDetect = async () => {
    if (!ownerId || !parkingLotId.trim() || !cameraId.trim()) {
      setError('Vui l√≤ng nh·∫≠p Parking Lot ID v√† Camera ID tr∆∞·ªõc khi detect');
      return;
    }

    if (status !== 'streaming') {
      setError('Vui l√≤ng b·∫Øt ƒë·∫ßu stream tr∆∞·ªõc khi detect');
      return;
    }
    
    setDetectingPlate(true);
    setError(null);
    
    try {
      // Capture frame from video
      const frameDataUrl = captureFrameFromVideo();
      if (!frameDataUrl) {
        throw new Error('Kh√¥ng th·ªÉ capture frame t·ª´ video');
      }
      
      // Send to plate detection API
      let response: Response;
      try {
        response = await fetch(`${API_BASE}/api/plate-detect`, {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({ imageData: frameDataUrl }),
        });
      } catch (fetchError) {
        throw new Error(`Network error: ${fetchError}`);
      }

      if (!response.ok) {
        const errorText = await response.text();
        throw new Error(`HTTP ${response.status}: ${errorText}`);
      }

      const data = await response.json();
      
      console.log('üì• Plate detection response:', {
        success: data.success,
        platesCount: data.plates?.length || 0,
        hasAnnotatedImage: !!data.annotatedImage,
        error: data.error,
      });
      
      if (!data.success) {
        throw new Error(data.error || 'Plate detection failed');
      }
      
      const detectedPlates: PlateResult[] = data.plates || [];
      
      console.log('üîç Detected plates:', detectedPlates);
      
      if (detectedPlates.length === 0) {
        setError('Kh√¥ng t√¨m th·∫•y bi·ªÉn s·ªë n√†o trong frame n√†y. H√£y th·ª≠ capture l·∫°i ho·∫∑c ƒë·∫£m b·∫£o bi·ªÉn s·ªë r√µ r√†ng trong video.');
        setDetectingPlate(false);
        return;
      }
      
      // Filter plates v·ªõi confidence th·∫•p (d∆∞·ªõi 10% - gi·∫£m threshold ƒë·ªÉ d·ªÖ detect h∆°n)
      // Ch·ªâ l·ªçc b·ªè text r·ªóng, kh√¥ng filter theo ƒë·ªô d√†i
      const validPlates = detectedPlates.filter(plate => {
        const conf = plate.confidence || 0;
        // Ch·ªâ c·∫ßn c√≥ text (kh√¥ng r·ªóng) v√† confidence >= 10%
        const hasValidText = plate.text && plate.text.trim().length > 0;
        return conf >= 0.1 && hasValidText;
      });
      
      if (validPlates.length === 0 && detectedPlates.length > 0) {
        // C√≥ detect nh∆∞ng confidence th·∫•p ho·∫∑c text kh√¥ng h·ª£p l·ªá
        const platesInfo = detectedPlates.map(p => `${p.text || '(empty)'} (${((p.confidence || 0) * 100).toFixed(1)}%)`).join(', ');
        setError(`T√¨m th·∫•y ${detectedPlates.length} bi·ªÉn s·ªë nh∆∞ng kh√¥ng ƒë·ªß tin c·∫≠y: ${platesInfo}. H√£y th·ª≠ capture l·∫°i khi bi·ªÉn s·ªë r√µ h∆°n.`);
        setDetectingPlate(false);
        return;
      }
      
      if (validPlates.length === 0) {
        // Hi·ªÉn th·ªã th√¥ng b√°o v·ªõi th√¥ng tin debug
        let errorMsg = 'Kh√¥ng t√¨m th·∫•y bi·ªÉn s·ªë n√†o trong frame n√†y.\n';
        if (detectedPlates.length > 0) {
          errorMsg += `Model ƒë√£ detect ${detectedPlates.length} k·∫øt qu·∫£ nh∆∞ng kh√¥ng ƒë·ªß ƒëi·ªÅu ki·ªán (confidence ho·∫∑c text kh√¥ng h·ª£p l·ªá).\n`;
          errorMsg += `Chi ti·∫øt: ${detectedPlates.map(p => `"${p.text || '(r·ªóng)'}" (${((p.confidence || 0) * 100).toFixed(1)}%)`).join(', ')}`;
        } else {
          errorMsg += 'H√£y th·ª≠ capture l·∫°i khi bi·ªÉn s·ªë r√µ r√†ng v√† ƒë·∫ßy ƒë·ªß trong video.';
        }
        setError(errorMsg);
        setDetectingPlate(false);
        return;
      }
      
      console.log('‚úÖ Valid plates (confidence >= 20%):', validPlates);
      
      // Compress input image ƒë·ªÉ l∆∞u v√†o Firebase (gi·∫£m k√≠ch th∆∞·ªõc)
      const compressedInputImage = compressImageDataUrl(frameDataUrl, 0.7);
      console.log('üìä Image sizes:', {
        original: `${(estimateDataUrlBytes(frameDataUrl) / 1024).toFixed(1)} KB`,
        compressedInput: `${(estimateDataUrlBytes(compressedInputImage) / 1024).toFixed(1)} KB`,
        annotatedImage: data.annotatedImage ? `${(estimateDataUrlBytes(data.annotatedImage) / 1024).toFixed(1)} KB` : 'N/A',
        note: 'Annotated image ch·ªâ hi·ªÉn th·ªã trong UI, kh√¥ng l∆∞u v√†o Firebase (qu√° l·ªõn)',
      });
      
      // Save each detected plate to Firebase
      // NOTE: Kh√¥ng l∆∞u annotatedImageUrl v√†o Firebase v√¨ qu√° l·ªõn (>1MB)
      // Annotated image ch·ªâ ƒë∆∞·ª£c l∆∞u trong local state ƒë·ªÉ hi·ªÉn th·ªã trong UI
      const now = new Date();
      const savedDetections: StreamPlateDetection[] = [];
      
      // S·ª≠ d·ª•ng validPlates thay v√¨ detectedPlates
      for (const plate of validPlates) {
        // Save to Firebase - CH·ªà l∆∞u inputImageUrl, KH√îNG l∆∞u annotatedImageUrl
        const saveResult = await savePlateDetection({
          ownerId,
          parkingId: parkingLotId.trim(),
          cameraId: cameraId.trim(),
          plateText: plate.text,
          confidence: plate.confidence,
          inputImageUrl: compressedInputImage,
          annotatedImageUrl: undefined,
          rawResponse: data.raw,
        });
        
        if (saveResult.success) {
          const detection: StreamPlateDetection = {
            id: saveResult.id || `detection-${Date.now()}-${Math.random()}`,
            plateText: plate.text,
            detectedAt: now,
            confidence: plate.confidence,
            inputImageUrl: compressedInputImage,
            parkingId: parkingLotId.trim(),
            annotatedImageUrl: data.annotatedImage || undefined,
```

#### **Filter Logic:**

- **Confidence threshold**: ‚â• 10% (r·∫•t th·∫•p ƒë·ªÉ d·ªÖ detect)
- **Text validation**: Ph·∫£i c√≥ text (kh√¥ng r·ªóng)
- **Kh√¥ng filter theo ƒë·ªô d√†i**: ƒê·ªÉ kh√¥ng b·ªè s√≥t bi·ªÉn s·ªë ng·∫Øn

---

### **3. Plate History Page: `frontend/src/pages/PlateHistoryPage.tsx`**

Hi·ªÉn th·ªã l·ªãch s·ª≠ t·∫•t c·∫£ plate detections:

```24:43:frontend/src/pages/PlateHistoryPage.tsx
  useEffect(() => {
    const loadRecords = async () => {
      if (!ownerId) {
        setRecords([]);
        setLoading(false);
        return;
      }
      setLoading(true);
      const result = await fetchPlateDetections({ ownerId });
      if (result.success && result.data) {
        setRecords(result.data);
        setError(null);
      } else {
        setRecords([]);
        setError(result.error || 'Kh√¥ng th·ªÉ t·∫£i d·ªØ li·ªáu');
      }
      setLoading(false);
    };
    loadRecords();
  }, [ownerId]);
```

**Features:**
- Filter theo parking lot
- Sort theo time, plate text, confidence, etc.
- Delete records
- View annotated images

---

## üìä **Data Flow Diagram**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  FRONTEND: StreamHostPage                               ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ  1. User clicks "Detect Plate"                         ‚îÇ
‚îÇ  2. captureFrameFromVideo() ‚Üí base64 image             ‚îÇ
‚îÇ  3. POST /api/plate-detect                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  BACKEND: FastAPI (/api/plate-detect)                  ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ  1. Receive { imageData: "base64..." }                  ‚îÇ
‚îÇ  2. Call ai_service.detect_plate()                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  AI SERVICE: ai_service.py                             ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ  1. Decode base64 ‚Üí OpenCV image                       ‚îÇ
‚îÇ  2. ALPR.predict(frame)                                ‚îÇ
‚îÇ     ‚îú‚îÄ Detector: Find plate location                   ‚îÇ
‚îÇ     ‚îî‚îÄ OCR: Recognize text                              ‚îÇ
‚îÇ  3. Annotate image (draw boxes + labels)                ‚îÇ
‚îÇ  4. Return { plates: [...], annotatedImage: "..." }     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  FRONTEND: Save to Firestore                           ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ  1. Filter valid plates (confidence >= 10%)            ‚îÇ
‚îÇ  2. For each plate:                                     ‚îÇ
‚îÇ     - savePlateDetection() ‚Üí Firestore                 ‚îÇ
‚îÇ     - Collection: plateDetections                      ‚îÇ
‚îÇ  3. Update UI v·ªõi annotated image                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üîß **Technical Details**

### **ALPR Model Architecture:**

```
Input Image
    ‚Üì
YOLO v9 Tiny 384 (Detector)
    ‚Üì
License Plate Bounding Boxes
    ‚Üì
Crop each plate region
    ‚Üì
Mobile ViT v2 (OCR)
    ‚Üì
Text Recognition
    ‚Üì
Output: { text, confidence, bbox }
```

### **Bounding Box Format:**

- **Backend (ALPR)**: `[x1, y1, x2, y2]` (absolute coordinates)
- **Frontend**: `[x, y, width, height]` (top-left + size)
- **Conversion**: `[x1, y1, x2-x1, y2-y1]`

### **Confidence Threshold:**

- **Minimum**: 10% (r·∫•t th·∫•p ƒë·ªÉ kh√¥ng b·ªè s√≥t)
- **Filter**: Ch·ªâ b·ªè qua plates kh√¥ng c√≥ text
- **Reason**: Bi·ªÉn s·ªë Vi·ªát Nam c√≥ nhi·ªÅu format kh√°c nhau

### **Image Processing:**

1. **Input**: Base64 encoded image (JPEG/PNG)
2. **Decode**: `base64.b64decode()` ‚Üí bytes
3. **OpenCV**: `cv2.imdecode()` ‚Üí numpy array
4. **ALPR**: Process ‚Üí Results
5. **Annotate**: Draw boxes + text
6. **Encode**: `cv2.imencode()` ‚Üí PNG ‚Üí base64

---

## üíæ **Firestore Structure**

### **Collection: `plateDetections`**

```typescript
{
  id: "auto-generated",
  ownerId: "yt88rSJpBsMzjnWX2SfCN687iex1",
  parkingId: "PARKING_A",
  cameraId: "CAM001",
  plateText: "30A-12345",
  confidence: 0.95,
  inputImageUrl: "data:image/jpeg;base64,...",  // Compressed
  annotatedImageUrl: undefined,  // NOT saved (too large)
  rawResponse: {...},  // Optional
  createdAt: Timestamp
}
```

**Note:** `annotatedImageUrl` kh√¥ng ƒë∆∞·ª£c l∆∞u v√†o Firestore v√¨ qu√° l·ªõn (>1MB). Ch·ªâ l∆∞u trong local state ƒë·ªÉ hi·ªÉn th·ªã.

---

## üéØ **Use Cases**

### **1. Real-time Detection t·ª´ Video Stream**
- User stream video t·ª´ camera
- Click "Detect Plate" ‚Üí Capture frame
- G·ª≠i frame ƒë·∫øn API ‚Üí Nh·∫≠n k·∫øt qu·∫£
- Hi·ªÉn th·ªã annotated image v·ªõi bounding boxes

### **2. Batch Processing**
- Upload nhi·ªÅu ·∫£nh
- Process t·ª´ng ·∫£nh
- L∆∞u t·∫•t c·∫£ results v√†o Firestore

### **3. History & Analytics**
- Xem l·ªãch s·ª≠ t·∫•t c·∫£ detections
- Filter theo parking lot, camera
- Sort theo confidence, time
- Export data

---

## ‚öôÔ∏è **Configuration**

### **API Endpoint:**
```typescript
// frontend/src/config/api.ts
endpoints: {
  plateDetect: '/api/plate-detect',
}
```

### **Model Names:**
```python
# server/services/ai_service.py
ALPR(
    detector_model="yolo-v9-t-384-license-plate-end2end",
    ocr_model="global-plates-mobile-vit-v2-model",
)
```

### **Confidence Threshold:**
```typescript
// frontend/src/pages/StreamHostPage.tsx
const validPlates = detectedPlates.filter(plate => {
  const conf = plate.confidence || 0;
  const hasValidText = plate.text && plate.text.trim().length > 0;
  return conf >= 0.1 && hasValidText;  // 10% minimum
});
```

---

## üêõ **Error Handling**

### **Backend Errors:**
- `Missing imageData` ‚Üí 400 Bad Request
- `Invalid base64 image` ‚Üí 400 Bad Request
- `Unable to decode image` ‚Üí 400 Bad Request
- `Failed to encode annotated image` ‚Üí 500 Internal Server Error

### **Frontend Errors:**
- Network errors ‚Üí Retry ho·∫∑c show error message
- No plates detected ‚Üí Show helpful message
- Low confidence ‚Üí Show confidence details
- Firebase save errors ‚Üí Log v√† show alert

---

## üìà **Performance**

### **Processing Time:**
- **ALPR Detection**: ~200-500ms per image (depends on image size)
- **Image Annotation**: ~50-100ms
- **Total**: ~250-600ms per request

### **Optimization:**
- Models loaded once (singleton pattern)
- Image compression before saving to Firestore
- Annotated images not saved (too large)

---

## üîÆ **Future Improvements**

1. **Multi-frame averaging**: Combine results t·ª´ nhi·ªÅu frames
2. **Plate format validation**: Validate Vietnamese plate format
3. **Tracking**: Link plates v·ªõi vehicle tracking IDs
4. **Real-time stream**: Process m·ªói N frames automatically
5. **Confidence calibration**: Fine-tune threshold per camera

---

**Updated:** December 5, 2025

