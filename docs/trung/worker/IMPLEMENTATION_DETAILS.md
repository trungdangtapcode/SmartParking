# Worker Architecture: ByteTrack Tracking Implementation

## Overview
This document details the implementation of the parking monitor worker with ByteTrack multi-object tracking, including how it processes camera feeds, performs tracking, and broadcasts results to the frontend.

## Table of Contents
1. [Architecture Overview](#architecture-overview)
2. [Worker Implementation](#worker-implementation)
3. [Tracking Pipeline](#tracking-pipeline)
4. [Backend Broadcast System](#backend-broadcast-system)
5. [Frontend Integration](#frontend-integration)
6. [Configuration System](#configuration-system)
7. [Performance Optimization](#performance-optimization)

---

## Architecture Overview

### System Flow
```
ESP32 Camera â†’ Worker (Tracking) â†’ FastAPI Server â†’ Frontend (Display)
     â†“              â†“                    â†“                â†“
  Capture      ByteTrack          Broadcast WS      React UI
  Stream       Detection          HTTP POST         WebSocket
```

### Key Components
1. **ParkingMonitorWorker** (`parking_monitor_worker.py`)
   - Fetches frames from cameras
   - Runs ByteTrack tracking
   - Matches vehicles to parking spaces
   - Broadcasts annotated frames

2. **AI Service** (`services/ai_service.py`)
   - YOLO v8 detection
   - ByteTrack tracking
   - Trail visualization

3. **Detection Broadcaster** (`services/detection_broadcaster.py`)
   - Manages WebSocket connections
   - Broadcasts frames to multiple viewers
   - Handles viewer disconnections

4. **Frontend** (`frontend/src/pages/DetectionViewerPage.tsx`)
   - Displays live tracking stream
   - Shows real-time statistics
   - Manages WebSocket connections

---

## Worker Implementation

### 1. Class Structure

**File:** `server/parking_monitor_worker.py`

```python
class ParkingMonitorWorker:
    def __init__(
        self,
        check_interval: float = 0.1,  # 10 FPS
        detection_url: str = "http://localhost:8069",
        update_firebase: bool = False,  # Disabled for performance
        enable_logging: bool = True,
        use_tracking: bool = True  # Enable ByteTrack
    ):
```

**Key Attributes:**
- `use_tracking`: Toggle between tracking and detection-only mode
- `tracking_config`: Loaded from `config/tracking_config.yaml`
- `ai_service`: Instance of AIService with YOLO + ByteTrack
- `camera_spaces_cache`: Cached parking space definitions
- `active_cameras_cache`: Cached list of cameras (30s TTL)

### 2. Initialization Flow

```python
async def start(self):
    # 1. Load AI models (YOLO + ALPR)
    await self.load_ai_models()
    
    # 2. Print tracking configuration
    if self.use_tracking:
        self.tracking_config.print_summary()
    
    # 3. Start monitoring loop
    await self.monitor_loop()
```

**AI Model Loading:**
```python
async def load_ai_models(self):
    logger.info("Loading AI models...")
    await self.ai_service.load_models()
    # Loads:
    # - YOLO v8 (yolov8s_car_custom.pt)
    # - Fast-ALPR (for license plate detection)
    # - Moves models to CUDA if available
```

### 3. Main Monitoring Loop

**File:** `parking_monitor_worker.py:monitor_loop()`

```python
async def monitor_loop(self):
    while self.is_running:
        # 1. Get active cameras (cached, 30s refresh)
        active_cameras = await self.get_active_cameras()
        
        # 2. Update worker status in Firebase
        await self.update_worker_status(camera_ids)
        
        # 3. Process all cameras in parallel
        tasks = [self.process_camera(camera) for camera in active_cameras]
        await asyncio.gather(*tasks, return_exceptions=True)
        
        # 4. Sleep for target FPS
        await asyncio.sleep(self.check_interval)  # 0.1s = 10 FPS
```

**Rate Limiting:**
- Each camera limited to `min_process_interval` (default: 0.1s = 10 FPS)
- Prevents overwhelming the system with requests
- Configurable via `--fps` command-line argument

---

## Tracking Pipeline

### 1. Camera Processing Flow

**File:** `parking_monitor_worker.py:process_camera()`

```python
async def process_camera(self, camera_info: Dict):
    # 1. Rate limiting check
    if current_time - last_time < self.min_process_interval:
        return  # Skip this frame
    
    # 2. Get parking spaces (cached)
    spaces = self.camera_spaces_cache.get(camera_id)
    
    # 3. Fetch frame from camera
    frame_bytes = await self.fetch_camera_frame(camera_url)
    
    # 4. Decode frame to numpy array
    frame = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
    
    # 5. Run tracking/detection
    detections = await self.detect_vehicles_in_frame(frame)
    
    # 6. Match detections to parking spaces
    space_occupancy = self.parking_service.match_detections_to_spaces(...)
    
    # 7. Draw annotations (boxes, trails, spaces)
    annotated_frame = self.draw_detections_on_frame(...)
    
    # 8. Broadcast to viewers
    await self.broadcast_frame_to_viewers(...)
    
    # 9. Log detections (async)
    await detection_logger.log_detection(...)
```

### 2. Vehicle Detection with Tracking

**File:** `parking_monitor_worker.py:detect_vehicles_in_frame()`

```python
async def detect_vehicles_in_frame(self, image_input) -> List[Dict]:
    # Decode image if bytes
    if isinstance(image_input, bytes):
        nparr = np.frombuffer(image_input, np.uint8)
        frame = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
    else:
        frame = image_input
    
    # Run tracking or detection
    if self.use_tracking:
        # ByteTrack tracking mode
        detections = await self.ai_service.detect_objects(
            frame,
            conf_threshold=self.tracking_config.conf_threshold,
            iou_threshold=self.tracking_config.iou_threshold,
            use_tracking=True  # Enable ByteTrack
        )
        logger.debug(f"ðŸŽ¯ Tracked {len(detections)} objects")
    else:
        # Detection only mode
        detections = await self.ai_service.detect_objects(
            frame,
            use_tracking=False
        )
        logger.debug(f"ðŸ” Detected {len(detections)} objects")
    
    # Filter for vehicles only
    vehicle_classes = ['car', 'truck', 'bus', 'motorcycle']
    vehicles = [d for d in detections if d.get('class') in vehicle_classes]
    
    return vehicles
```

**Detection Format:**
```python
{
    'class': 'car',
    'confidence': 0.85,
    'bbox': [x, y, width, height],  # Pixels
    'track_id': 42  # Only if tracking enabled
}
```

### 3. AI Service Integration

**File:** `services/ai_service.py:detect_objects()`

```python
async def detect_objects(
    self,
    frame: np.ndarray,
    conf_threshold: float = 0.25,
    iou_threshold: float = 0.45,
    use_tracking: bool = False
) -> list:
    self.frame_count += 1
    
    if use_tracking:
        # ByteTrack tracking
        results = self.yolo_model.track(
            source=frame,
            conf=conf_threshold,
            iou=iou_threshold,
            persist=True,  # Keep track IDs between frames
            verbose=False,
            device=self.device,
            tracker="bytetrack.yaml"  # Use ByteTrack algorithm
        )
    else:
        # Detection only
        results = self.yolo_model.predict(
            source=frame,
            conf=conf_threshold,
            iou=iou_threshold,
            verbose=False,
            device=self.device
        )
    
    detections = []
    for result in results:
        boxes = result.boxes
        if boxes is None:
            continue
        
        for box in boxes:
            # Extract bbox coordinates
            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
            
            detection = {
                'class': result.names[int(box.cls[0])],
                'confidence': float(box.conf[0]),
                'bbox': [float(x1), float(y1), float(x2-x1), float(y2-y1)]
            }
            
            # Add track_id if tracking enabled
            if use_tracking and hasattr(box, 'id') and box.id is not None:
                track_id = int(box.id[0])
                detection['track_id'] = track_id
                
                # Update track history for trail visualization
                center_x = int((x1 + x2) / 2)
                center_y = int((y1 + y2) / 2)
                
                if track_id not in self.track_history:
                    self.track_history[track_id] = []
                
                self.track_history[track_id].append((center_x, center_y))
                
                # Keep only last 30 points
                if len(self.track_history[track_id]) > 30:
                    self.track_history[track_id].pop(0)
            
            detections.append(detection)
    
    return detections
```

### 4. Frame Annotation

**File:** `parking_monitor_worker.py:draw_detections_on_frame()`

```python
def draw_detections_on_frame(
    self,
    frame,
    detections: List[Dict],
    parking_spaces: List[Dict],
    space_occupancy: Dict[str, bool],
    image_width: int,
    image_height: int
):
    import cv2
    
    # If tracking enabled, use AI service's draw function
    if self.use_tracking:
        # Draw detections with tracking trails
        frame = self.ai_service.draw_detections(
            frame,
            detections,
            show_trails=self.tracking_config.get('visualization.show_trail', True),
            show_track_id=self.tracking_config.get('visualization.show_track_id', True)
        )
    else:
        # Draw simple bounding boxes
        for detection in detections:
            bbox = detection['bbox']
            x1, y1 = int(bbox[0]), int(bbox[1])
            x2, y2 = int(bbox[0] + bbox[2]), int(bbox[1] + bbox[3])
            
            cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 0), 2)
            
            label = f"{detection.get('class')}: {detection.get('confidence', 0):.2f}"
            cv2.putText(frame, label, (x1, y1 - 10), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1)
    
    # Draw parking spaces
    for space in parking_spaces:
        space_id = space['id']
        is_occupied = space_occupancy.get(space_id, False)
        
        # Convert normalized coordinates to pixels
        x = int(space['x'] * image_width)
        y = int(space['y'] * image_height)
        w = int(space['width'] * image_width)
        h = int(space['height'] * image_height)
        
        # Color: Red if occupied, Green if free
        color = (0, 0, 255) if is_occupied else (0, 255, 0)
        
        # Draw rectangle
        cv2.rectangle(frame, (x, y), (x + w, y + h), color, 3)
        
        # Draw label
        label = f"{space.get('name', space_id)}: {'Occupied' if is_occupied else 'Free'}"
        cv2.putText(frame, label, (x + 3, y - 5), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
    
    return frame
```

**AI Service Draw Function:**

**File:** `services/ai_service.py:draw_detections()`

```python
def draw_detections(
    self,
    frame: np.ndarray,
    detections: List[Dict],
    show_trails: bool = True,
    show_track_id: bool = True
) -> np.ndarray:
    annotated = frame.copy()
    
    for det in detections:
        x, y, w, h = det['bbox']
        x1, y1, x2, y2 = int(x), int(y), int(x + w), int(y + h)
        
        class_name = det['class']
        confidence = det['confidence']
        track_id = det.get('track_id')
        
        # Color based on track ID
        if track_id is not None:
            color = self._get_track_color(track_id)
        else:
            color = (0, 255, 0)
        
        # Draw bounding box
        cv2.rectangle(annotated, (x1, y1), (x2, y2), color, 2)
        
        # Draw label
        if show_track_id and track_id is not None:
            label = f"ID:{track_id} {class_name} {confidence:.2f}"
        else:
            label = f"{class_name} {confidence:.2f}"
        
        (tw, th), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)
        cv2.rectangle(annotated, (x1, y1 - th - 8), (x1 + tw + 6, y1), color, -1)
        cv2.putText(annotated, label, (x1 + 3, y1 - 5), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        
        # Draw tracking trail
        if show_trails and track_id is not None and track_id in self.track_history:
            points = self.track_history[track_id]
            if len(points) > 1:
                for i in range(1, len(points)):
                    cv2.line(annotated, points[i-1], points[i], color, 2)
    
    return annotated

def _get_track_color(self, track_id: int) -> tuple:
    """Generate consistent color for each track ID"""
    np.random.seed(track_id)
    color = tuple(np.random.randint(50, 255, 3).tolist())
    np.random.seed()  # Reset seed
    return color
```

---

## Backend Broadcast System

### 1. Frame Broadcasting

**File:** `parking_monitor_worker.py:broadcast_frame_to_viewers()`

```python
async def broadcast_frame_to_viewers(
    self, 
    camera_id: str, 
    frame, 
    metadata: dict
):
    import cv2
    import base64
    
    try:
        # Encode frame to JPEG
        _, buffer = cv2.imencode('.jpg', frame, [cv2.IMWRITE_JPEG_QUALITY, 85])
        frame_base64 = base64.b64encode(buffer).decode('utf-8')
        
        # Send to FastAPI server via HTTP POST
        async with aiohttp.ClientSession() as session:
            broadcast_url = f'{self.detection_url}/api/broadcast-detection'
            payload = {
                'camera_id': camera_id,
                'frame_base64': frame_base64,
                'metadata': metadata
            }
            
            async with session.post(
                broadcast_url, 
                json=payload, 
                timeout=aiohttp.ClientTimeout(total=5)
            ) as response:
                if response.status == 200:
                    logger.debug(f"ðŸ“º Sent frame to FastAPI for camera {camera_id}")
                else:
                    logger.warning(f"Failed to send frame: HTTP {response.status}")
    
    except asyncio.TimeoutError:
        logger.warning(f"Timeout sending frame to FastAPI")
    except Exception as e:
        logger.error(f"Error broadcasting frame: {e}")
```

**Metadata Structure (Detailed Tracking Information):**
```python
tracking_info = {
    # Summary statistics
    "vehicle_count": len(detections),
    "occupied_spaces": sum(space_occupancy.values()),
    "total_spaces": len(spaces),
    "timestamp": datetime.now().isoformat(),
    
    # Detailed detection information
    "detections": [
        {
            "class": "car",
            "confidence": 0.87,
            "bbox": [x, y, width, height],  # Pixels
            "track_id": 42,  # None if tracking disabled
            "center": [center_x, center_y]  # Center point of bbox
        }
    ],
    
    # Parking space occupancy details
    "space_occupancy": [
        {
            "space_id": "space_1",
            "space_name": "A1",
            "is_occupied": True,
            "bbox": {
                "x": 0.1,  # Normalized coordinates
                "y": 0.2,
                "width": 0.15,
                "height": 0.2
            }
        }
    ],
    
    # Matched detections (which vehicle in which space)
    "matched_detections": [
        {
            "detection": {
                "class": "car",
                "confidence": 0.87,
                "bbox": [x, y, width, height],
                "track_id": 42
            },
            "space_id": "space_1"
        }
    ],
    
    # Configuration flags
    "tracking_enabled": True  # Whether ByteTrack is enabled
}
```

### 2. Detection Broadcaster Service

**File:** `server/routers/worker_broadcast.py`

```python
@router.post("/broadcast-detection")
async def broadcast_detection(request: BroadcastRequest):
    """
    Receive detection frame from worker and broadcast to viewers
    
    Flow:
    1. Worker sends frame + metadata
    2. Store in broadcaster
    3. Broadcast to all WebSocket viewers
    """
    try:
        # Decode base64 image
        frame_bytes = base64.b64decode(request.frame_base64)
        
        # Get or create broadcaster for this camera
        broadcaster = detection_broadcaster.get_broadcaster(request.camera_id)
        
        # Update frame
        await broadcaster.update_frame(
            frame_bytes=frame_bytes,
            metadata=request.metadata
        )
        
        return {"success": True, "viewers": broadcaster.viewer_count}
    
    except Exception as e:
        logger.error(f"Error broadcasting detection: {e}")
        raise HTTPException(status_code=500, detail=str(e))
```

**File:** `server/services/detection_broadcaster.py`

```python
class DetectionBroadcaster:
    def __init__(self, camera_id: str):
        self.camera_id = camera_id
        self.current_frame: Optional[bytes] = None
        self.metadata: Dict = {}
        self.viewers: Set[WebSocket] = set()
        self.last_update = time.time()
        self.frame_count = 0
    
    async def update_frame(self, frame_bytes: bytes, metadata: Dict):
        """Update current frame and broadcast to all viewers"""
        self.current_frame = frame_bytes
        self.metadata = metadata
        self.last_update = time.time()
        self.frame_count += 1
        
        # Broadcast to all connected viewers
        await self.broadcast_to_viewers()
    
    async def broadcast_to_viewers(self):
        """Send current frame to all WebSocket viewers"""
        if not self.current_frame or not self.viewers:
            return
        
        # Encode frame to base64 for WebSocket
        frame_base64 = base64.b64encode(self.current_frame).decode('utf-8')
        
        # Prepare message
        message = {
            "type": "frame",
            "camera_id": self.camera_id,
            "frame": f"data:image/jpeg;base64,{frame_base64}",
            "metadata": self.metadata,
            "frame_count": self.frame_count,
            "timestamp": time.time()
        }
        
        # Send to all viewers (with error handling)
        disconnected = []
        for viewer in self.viewers:
            try:
                await asyncio.wait_for(
                    viewer.send_json(message),
                    timeout=0.5  # 500ms timeout per viewer
                )
            except (asyncio.TimeoutError, Exception) as e:
                logger.warning(f"Failed to send to viewer: {e}")
                disconnected.append(viewer)
        
        # Remove disconnected viewers
        for viewer in disconnected:
            self.viewers.discard(viewer)
    
    async def add_viewer(self, websocket: WebSocket):
        """Add new viewer to broadcaster"""
        await websocket.accept()
        self.viewers.add(websocket)
        logger.info(f"Viewer added to {self.camera_id}: {len(self.viewers)} total")
        
        # Send current frame immediately if available
        if self.current_frame:
            await self.send_frame_to_viewer(websocket)
    
    def remove_viewer(self, websocket: WebSocket):
        """Remove viewer from broadcaster"""
        self.viewers.discard(websocket)
        logger.info(f"Viewer removed from {self.camera_id}: {len(self.viewers)} remaining")
    
    @property
    def viewer_count(self) -> int:
        return len(self.viewers)
```

### 3. WebSocket Endpoint

**File:** `server/routers/worker_detection_stream.py`

```python
@router.websocket("/ws/viewer/detection")
async def viewer_websocket(websocket: WebSocket, camera_id: str):
    """
    WebSocket endpoint for viewers to receive detection streams
    
    Client connects â†’ Broadcaster adds viewer â†’ Receives frames
    """
    broadcaster = detection_broadcaster.get_broadcaster(camera_id)
    
    try:
        # Add viewer to broadcaster
        await broadcaster.add_viewer(websocket)
        
        # Keep connection alive
        while True:
            try:
                # Wait for client messages (ping/pong)
                data = await asyncio.wait_for(
                    websocket.receive_text(),
                    timeout=30.0
                )
                
                # Handle client messages
                if data == "ping":
                    await websocket.send_text("pong")
            
            except asyncio.TimeoutError:
                # No message received in 30s - check if still connected
                try:
                    await websocket.send_text("keepalive")
                except:
                    break
    
    except WebSocketDisconnect:
        logger.info(f"Viewer disconnected from {camera_id}")
    
    finally:
        # Remove viewer from broadcaster
        broadcaster.remove_viewer(websocket)
```

---

## Frontend Integration

### 1. Detection Viewer Page

**File:** `frontend/src/pages/DetectionViewerPage.tsx`

```typescript
export function DetectionViewerPage() {
  const [selectedCamera, setSelectedCamera] = useState<string>('');
  const [frameData, setFrameData] = useState<string>('');
  const [metadata, setMetadata] = useState<any>(null);
  const [isConnected, setIsConnected] = useState(false);
  const wsRef = useRef<WebSocket | null>(null);
  
  // ... camera loading logic ...
  
  // Connect to WebSocket when camera selected
  useEffect(() => {
    if (!selectedCamera) return;
    
    connectWebSocket(selectedCamera);
    
    return () => {
      disconnectWebSocket();
    };
  }, [selectedCamera]);
  
  const connectWebSocket = (cameraId: string) => {
    const ws = new WebSocket(
      `ws://localhost:8069/ws/viewer/detection?camera_id=${cameraId}`
    );
    
    ws.onopen = () => {
      console.log('âœ… WebSocket connected');
      setIsConnected(true);
    };
    
    ws.onmessage = (event) => {
      const data = JSON.parse(event.data);
      
      if (data.type === 'frame') {
        // Update frame image
        setFrameData(data.frame);  // Base64 data URL
        
        // Update metadata
        setMetadata(data.metadata);
      }
    };
    
    ws.onerror = (error) => {
      console.error('âŒ WebSocket error:', error);
      setIsConnected(false);
    };
    
    ws.onclose = () => {
      console.log('ðŸ”Œ WebSocket closed');
      setIsConnected(false);
    };
    
    wsRef.current = ws;
    
    // Send ping every 10 seconds to keep alive
    const pingInterval = setInterval(() => {
      if (ws.readyState === WebSocket.OPEN) {
        ws.send('ping');
      }
    }, 10000);
    
    return () => {
      clearInterval(pingInterval);
    };
  };
  
  const disconnectWebSocket = () => {
    if (wsRef.current) {
      wsRef.current.close();
      wsRef.current = null;
    }
    setIsConnected(false);
    setFrameData('');
    setMetadata(null);
  };
  
  return (
    <div className="detection-viewer">
      {/* Camera selector */}
      <select onChange={(e) => setSelectedCamera(e.target.value)}>
        {cameras.map(cam => (
          <option key={cam.id} value={cam.id}>{cam.name}</option>
        ))}
      </select>
      
      {/* Video display */}
      {frameData && (
        <img 
          src={frameData} 
          alt="Detection Stream" 
          className="w-full"
        />
      )}
      
      {/* Statistics */}
      {metadata && (
        <div className="stats">
          <div>Vehicles: {metadata.vehicle_count}</div>
          <div>Occupied: {metadata.occupied_spaces}/{metadata.total_spaces}</div>
          <div>Available: {metadata.total_spaces - metadata.occupied_spaces}</div>
        </div>
      )}
      
      {/* Connection status */}
      <div className={isConnected ? 'status-connected' : 'status-disconnected'}>
        {isConnected ? 'ðŸŸ¢ Connected' : 'ðŸ”´ Disconnected'}
      </div>
    </div>
  );
}
```

### 2. WebSocket Message Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     POST Frame      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Worker    â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> â”‚   FastAPI    â”‚
â”‚  (Tracker)  â”‚   HTTP /broadcast  â”‚   Server     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                           â”‚
                                           â”‚ Store in
                                           â”‚ Broadcaster
                                           â†“
                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                    â”‚  Detection   â”‚
                                    â”‚ Broadcaster  â”‚
                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                           â”‚
                                           â”‚ Broadcast
                                           â”‚ WebSocket
                                           â†“
                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                              â”‚   Multiple Viewers      â”‚
                              â”‚  (React Components)     â”‚
                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Configuration System

### 1. Tracking Configuration

**File:** `server/config/tracking_config.yaml`

```yaml
tracking:
  enabled: true
  tracker: "bytetrack"
  bytetrack:
    track_high_thresh: 0.5    # High confidence tracks
    track_low_thresh: 0.1     # Low confidence tracks
    new_track_thresh: 0.6     # Create new track threshold
    track_buffer: 30          # Frames to keep lost tracks
    match_thresh: 0.8         # IOU threshold for matching
  
detection:
  conf_threshold: 0.25        # Detection confidence
  iou_threshold: 0.45         # NMS IOU threshold
  classes: [2, 3, 5, 7]       # car, motorcycle, bus, truck
  
performance:
  fps: 10                     # Target FPS
  skip_frames: 0              # Skip N frames
  imgsz: 640                  # Input image size
  
visualization:
  show_track_id: true         # Show track IDs
  show_trail: true            # Show tracking trails
  trail_length: 30            # Points in trail
```

### 2. Configuration Loader

**File:** `server/utils/tracking_config.py`

```python
class TrackingConfig:
    def __init__(self, config_path: str = None):
        if config_path is None:
            self.config_path = Path(__file__).parent.parent / "config" / "tracking_config.yaml"
        else:
            self.config_path = Path(config_path)
        
        self.config = self._load_config()
    
    def _load_config(self) -> Dict[str, Any]:
        if not self.config_path.exists():
            logger.warning(f"Config file not found, using defaults")
            return self._get_default_config()
        
        with open(self.config_path, 'r') as f:
            config = yaml.safe_load(f)
        
        return config
    
    def get(self, key_path: str, default=None):
        """Get config value by dot-separated path"""
        keys = key_path.split('.')
        value = self.config
        
        for key in keys:
            if isinstance(value, dict) and key in value:
                value = value[key]
            else:
                return default
        
        return value

# Global singleton
def get_tracking_config() -> TrackingConfig:
    global _config_instance
    if _config_instance is None:
        _config_instance = TrackingConfig()
    return _config_instance
```

---

## Performance Optimization

### 1. Caching Strategy

**Camera List Caching:**
```python
# Cache camera list for 30 seconds
self.active_cameras_cache: List[Dict] = []
self.last_cameras_refresh: float = 0
self.cameras_refresh_interval: float = 30.0

# Only refresh if cache expired
if current_time - self.last_cameras_refresh < self.cameras_refresh_interval:
    return self.active_cameras_cache
```

**Parking Spaces Caching:**
```python
# Cache parking spaces per camera (never expires unless cleared)
self.camera_spaces_cache: Dict[str, List[Dict]] = {}

if camera_id not in self.camera_spaces_cache:
    spaces = self.parking_service.get_parking_spaces_by_camera(camera_id)
    self.camera_spaces_cache[camera_id] = spaces
```

### 2. Rate Limiting

```python
# Limit processing per camera
self.last_processed: Dict[str, float] = {}
self.min_process_interval: float = 0.1  # 10 FPS max

# Check if enough time passed
if current_time - last_time < self.min_process_interval:
    return  # Skip this frame
```

### 3. Parallel Processing

```python
# Process all cameras in parallel
tasks = [self.process_camera(camera) for camera in active_cameras]
await asyncio.gather(*tasks, return_exceptions=True)
```

### 4. Firebase Optimization

**Disabled by Default:**
```python
update_firebase: bool = False  # Disabled for high FPS

# Only update if explicitly enabled
if self.update_firebase:
    self.parking_service.update_space_occupancy(...)
```

**Why Disabled:**
- Firebase writes are slow (300-500ms)
- Reduces FPS from 10 to ~0.3
- Use detection logging instead (async, non-blocking)

### 5. GPU Acceleration

```python
# YOLO model on GPU
self.yolo_model.to(self.device)  # 'cuda' if available

# FP16 inference for 2x speed
results = self.yolo_model.predict(
    frame,
    half=True,  # Use FP16
    device='cuda'
)
```

---

## Command Line Usage

### Start Worker with Tracking

```bash
# Basic usage (10 FPS, tracking enabled)
python parking_monitor_worker.py

# High FPS mode
python parking_monitor_worker.py --fps 20

# High accuracy mode
python parking_monitor_worker.py --fps 5

# Disable tracking (detection only)
python parking_monitor_worker.py --no-tracking

# Enable Firebase updates (slow!)
python parking_monitor_worker.py --update-firebase

# Disable logging
python parking_monitor_worker.py --no-logging

# Debug mode
python parking_monitor_worker.py --debug
```

### Arguments

| Argument | Type | Default | Description |
|----------|------|---------|-------------|
| `--fps` | int | 10 | Target FPS per camera |
| `--interval` | float | 0.1 | Check interval in seconds |
| `--detection-url` | str | http://localhost:8069 | FastAPI server URL |
| `--update-firebase` | flag | False | Enable Firebase updates |
| `--no-logging` | flag | False | Disable detection logging |
| `--no-tracking` | flag | False | Disable ByteTrack tracking |
| `--debug` | flag | False | Enable debug logging |

---

## Monitoring and Debugging

### Console Output

**Tracking Mode:**
```
ðŸŽ¯ ByteTrack tracking ENABLED
==================================================
ðŸŽ¯ TRACKING CONFIGURATION
==================================================
Tracking Enabled: True
Tracker Type: bytetrack
Confidence Threshold: 0.25
Target FPS: 10
Device: cuda
==================================================
ðŸš€ Starting parking monitor worker...
â±ï¸  Target FPS: 10 FPS per camera
âœ… AI models loaded
ðŸ”„ Refreshing camera list from Firebase...
Found 2 parking lots
âœ… Loaded 4 parking spaces for camera CAM1
ðŸŽ¯ Tracked 3 objects (with track IDs)
  Track IDs: [42, 43, 44]
ðŸ“º Sent frame to FastAPI for camera CAM1
```

**Detection Only Mode:**
```
ðŸ” Detection only mode (tracking disabled)
ðŸ” Detected 3 objects (no tracking)
```

### Log Files

**Detection Logs:** `server/logs/detections/detection_{camera_id}_{date}.log`

```json
{
  "timestamp": "2026-01-07T12:30:45.123456",
  "camera_id": "CAM1",
  "detections": [
    {
      "class": "car",
      "confidence": 0.87,
      "bbox": [100, 200, 150, 100],
      "track_id": 42
    }
  ],
  "occupancy": {
    "space_1": true,
    "space_2": false
  },
  "stats": {
    "vehicle_count": 1,
    "occupied_spaces": 1,
    "total_spaces": 4
  }
}
```

---

## Summary

### Key Features
1. âœ… **ByteTrack Tracking** - Persistent track IDs across frames
2. âœ… **Real-time Broadcasting** - Worker â†’ FastAPI â†’ Frontend via WebSocket
3. âœ… **High Performance** - 10 FPS per camera with GPU acceleration
4. âœ… **Configurable** - YAML-based configuration system
5. âœ… **Scalable** - Parallel processing of multiple cameras
6. âœ… **Cached** - Smart caching to reduce Firebase queries
7. âœ… **Monitored** - Comprehensive logging and debugging

### Data Flow
```
Camera â†’ Worker.fetch_frame()
      â†’ Worker.detect_vehicles_in_frame()
      â†’ AIService.detect_objects(use_tracking=True)
      â†’ YOLOv8.track(tracker="bytetrack")
      â†’ Worker.draw_detections_on_frame()
      â†’ Worker.broadcast_frame_to_viewers()
      â†’ FastAPI.broadcast_detection()
      â†’ DetectionBroadcaster.update_frame()
      â†’ DetectionBroadcaster.broadcast_to_viewers()
      â†’ WebSocket â†’ Frontend
```

### Performance Metrics
- **Target FPS:** 10 per camera
- **Processing Time:** 45-60ms per frame (with CUDA)
- **Broadcast Latency:** < 100ms
- **Firebase Reads:** 2,880/day (with 30s cache)
- **Memory Usage:** ~2GB (with YOLO on GPU)

---

**Last Updated:** January 7, 2026
**Version:** 1.0
**Author:** Smart Parking System Team
